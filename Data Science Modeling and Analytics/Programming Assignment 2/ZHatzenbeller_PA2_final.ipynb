{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf78150-6df4-4101-9ed2-715fa2f64bed",
   "metadata": {},
   "source": [
    "## __System Information, Packages, and Data Loading__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b79a2-d6f8-40af-90e9-864ee8b2d039",
   "metadata": {},
   "source": [
    "### __Package Imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c9e41a-8f36-465f-a197-0053760aa5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "import psutil\n",
    "import platform\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from rouge_score import rouge_scorer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from nltk.translate.bleu_score import (\n",
    "    sentence_bleu, \n",
    "    corpus_bleu, \n",
    "    SmoothingFunction\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab386e70-7cb1-450b-9d64-d100b84977ff",
   "metadata": {},
   "source": [
    "### __Variable Definitions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f47b97f6-a506-4ff4-b22f-ca8f77544896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Variables\n",
    "TRAIN_SAVE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc5afe6-a475-4bab-ba0f-94ee41e33bd4",
   "metadata": {},
   "source": [
    "### __Software and Hardware Specs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ed2a39-8d10-4ad0-81f0-aed25d68fab1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "HARDWARE INFO\n",
      "==================================================\n",
      "OS: Linux 5.10.0-36-cloud-amd64\n",
      "Python: 3.10.19\n",
      "PyTorch: 2.9.1+cu128\n",
      "\n",
      "CPU: 32 cores\n",
      "RAM: 125.8 GB\n",
      "\n",
      "GPU: NVIDIA L4\n",
      "CUDA: 12.8\n",
      "GPU Memory: 22.0 GB\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"HARDWARE INFO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# System\n",
    "print(f\"OS: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "\n",
    "# CPU & Memory\n",
    "print(f\"\\nCPU: {psutil.cpu_count(logical=True)} cores\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "else:\n",
    "    print(\"\\nGPU: None (using CPU)\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54672f90-e9be-41a3-b76a-4c88e4e9195c",
   "metadata": {},
   "source": [
    "### __Download Flickr Dataset and Load Base Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1897be97-e3ee-44ba-9170-f93547ed8e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/jupyter/.cache/kagglehub/datasets/hsankesara/flickr-image-dataset/versions/1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"hsankesara/flickr-image-dataset\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e5653-37fa-4d6c-b077-f08e3abf6cc3",
   "metadata": {},
   "source": [
    "## __Model Selection and Customization__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95452060-d267-4f48-a991-527d1266cb63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-trained BLIP model\n",
    "model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
    "processor = BlipProcessor.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdf78ec-fd03-48ac-803f-753a10adfe64",
   "metadata": {},
   "source": [
    "## __Dataset Preparation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09a13e20-3a54-44bf-86c3-069e2046a870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. LOAD THE DATASET FROM DISK\n",
    "# ============================================\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, csv_file, processor):\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations = pd.read_csv(csv_file, sep=\"|\")\n",
    "        # Strip whitespace from column names\n",
    "        self.annotations.columns = self.annotations.columns.str.strip()\n",
    "        self.processor = processor\n",
    "\n",
    "        # group captions by image (BLIP expects 1 caption per sample)\n",
    "        self.data = (\n",
    "            self.annotations.groupby(\"image_name\")[\"comment\"]\n",
    "            .apply(list)\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_name = row[\"image_name\"]\n",
    "        captions = row[\"comment\"]\n",
    "\n",
    "        # Use the first caption (simplest for training)\n",
    "        caption = captions[0]\n",
    "\n",
    "        # Load image\n",
    "        image_path = os.path.join(self.root_dir, img_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Preprocess\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=40,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Remove batch dimension and prepare labels\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        \n",
    "        # BLIP needs labels for training\n",
    "        encoding[\"labels\"] = encoding[\"input_ids\"].clone()\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "# ============================================\n",
    "# 2. CUSTOM COLLATOR\n",
    "# ============================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collator to handle BLIP's expected input format\"\"\"\n",
    "    # Stack all tensors\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 3. CREATE DATASET OBJECTS\n",
    "# ============================================\n",
    "\n",
    "dataset_root = Path(path) / \"flickr30k_images\" / \"flickr30k_images\"\n",
    "csv_path = Path(path) / \"flickr30k_images\" / \"results.csv\"\n",
    "\n",
    "full_dataset = FlickrDataset(dataset_root, csv_path, processor)\n",
    "\n",
    "# Split manually\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1293afb-9450-4725-9dc9-4df2cf8b758b",
   "metadata": {},
   "source": [
    "## __Training and Fine-tuning__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd82781d-72e8-48de-a860-cc720cca319b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21453' max='21453' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21453/21453 1:19:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.269600</td>\n",
       "      <td>1.286147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.038700</td>\n",
       "      <td>1.238511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.704300</td>\n",
       "      <td>1.274218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['text_decoder.cls.predictions.decoder.weight', 'text_decoder.cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete and model saved!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. TRAINING ARGUMENTS\n",
    "# ============================================\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blip-finetuned-flickr\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,  # Only keep 2 best checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2. CUSTOM TRAINER TO HANDLE BLIP\n",
    "# ============================================\n",
    "\n",
    "class BlipTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"Override to remove num_items_in_batch argument that BLIP doesn't expect\"\"\"\n",
    "        # Remove any keys that BLIP doesn't need\n",
    "        inputs = {k: v for k, v in inputs.items() if k in [\"pixel_values\", \"input_ids\", \"attention_mask\", \"labels\"]}\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "if TRAIN_SAVE:\n",
    "    # Define the model trainer\n",
    "    trainer = BlipTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=collate_fn,  # custom collator\n",
    "    )\n",
    "    # ============================================\n",
    "    # 3. TRAIN\n",
    "    # ============================================\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # ============================================\n",
    "    # 4. SAVE MODEL \n",
    "    # ============================================\n",
    "\n",
    "    trainer.save_model(\"./blip-finetuned-flickr\")\n",
    "    processor.save_pretrained(\"./blip-finetuned-flickr\")\n",
    "\n",
    "    print(\"Training complete and model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c6c40c-128c-40ac-85cc-6851587ba5b8",
   "metadata": {},
   "source": [
    "## __Model Evaluation and Comparison__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26cdcaf-c9ad-4ee0-bac6-bd13dda97abe",
   "metadata": {},
   "source": [
    "### __Load and Evaluate Finetuned Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f358e04f-eb3a-4711-bc80-29050e2f9d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and processor...\n",
      "Model loaded on cuda\n",
      "\n",
      "Evaluating on 10 images...\n",
      "\n",
      "Image: 3655176735.jpg\n",
      "Generated:    a woman in a white shirt is reading a picture book to a man in a military uniform sitting in a chair.\n",
      "Ground truth:  AN older woman appears to read from a children 's book in an indoor setting , while a seated gentleman in a service uniform looks on .\n",
      "\n",
      "Image: 7669392800.jpg\n",
      "Generated:    a group of bicyclists race down a street lined with bushes.\n",
      "Ground truth:  Numerous bicyclists wearing bicyclist apparel , helmets , goggles , and gloves racing fiercely down a paved road despite the rain .\n",
      "\n",
      "Image: 4546029322.jpg\n",
      "Generated:    many people are walking on the street with orange and white traffic cones.\n",
      "Ground truth:  In this picture we see multiple people crossing a courtyard with a line of traffic cones dissecting it .\n",
      "\n",
      "Image: 120764850.jpg\n",
      "Generated:    a little boy and a little girl are laughing while sitting on the floor.\n",
      "Ground truth:  A baby on the floor laughing at an older another child .\n",
      "\n",
      "Image: 2912210904.jpg\n",
      "Generated:    a man in a black t - shirt is pushing a cart filled with tree branches.\n",
      "Ground truth:  A man loads trees onto a cart on the side of a road .\n",
      "\n",
      "Image: 7645700076.jpg\n",
      "Generated:    a bicycle race is taking place on a cobblestone street.\n",
      "Ground truth:  cyclists lean into a turn , behind them is a barricade and behind that are people watching the cycling race .\n",
      "\n",
      "Image: 512163695.jpg\n",
      "Generated:    a man with a white shirt and backpack is walking down a sidewalk next to a red bench.\n",
      "Ground truth:  A male in gray pants with a blue shirt and backpack walking down a sidewalk in a neighborhood .\n",
      "\n",
      "Image: 4604111022.jpg\n",
      "Generated:    an older man in a striped shirt is looking through a microscope.\n",
      "Ground truth:  An elderly man in a white and gray striped shirt eating food .\n",
      "\n",
      "Image: 2326730558.jpg\n",
      "Generated:    two dogs, one white and one brown, are playing in the snow.\n",
      "Ground truth:  A brown dog kisses the ear of a white dog in the snow .\n",
      "\n",
      "Image: 2905948395.jpg\n",
      "Generated:    a man is performing a trick on a bicycle in a skate and cycle park.\n",
      "Ground truth:  A man does bike jumps in the dark in an empty pool .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. LOAD YOUR FINE-TUNED MODEL \n",
    "# ============================================\n",
    "\n",
    "model_path = \"./blip-finetuned-flickr\"\n",
    "image_path = Path(path) / \"flickr30k_images\" / \"flickr30k_images\"\n",
    "csv_path = Path(path) / \"flickr30k_images\" / \"results.csv\"\n",
    "\n",
    "print(\"Loading model and processor...\")\n",
    "processor = BlipProcessor.from_pretrained(model_path)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# ============================================\n",
    "# 2. GENERATE CAPTION FOR A SINGLE IMAGE\n",
    "# ============================================\n",
    "\n",
    "def generate_caption(image_path, max_length=50, num_beams=5):\n",
    "    \"\"\"\n",
    "    Generate a caption for a single image\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        max_length: Maximum length of generated caption\n",
    "        num_beams: Number of beams for beam search (higher = better quality, slower)\n",
    "    \n",
    "    Returns:\n",
    "        Generated caption string\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Process image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate caption\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode the generated caption\n",
    "    caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 3. EVALUATE ON FLICKR TEST SET\n",
    "# ============================================\n",
    "\n",
    "def evaluate_on_flickr(csv_path, images_dir, num_samples=100):\n",
    "    \"\"\"Evaluate the model on Flickr30k test images with ground truth captions\"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load captions\n",
    "    df = pd.read_csv(csv_path, sep=\"|\")\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Group by image\n",
    "    grouped = df.groupby(\"image_name\")[\"comment\"].apply(list).reset_index()\n",
    "    \n",
    "    # Sample some images\n",
    "    sample_data = grouped.sample(min(num_samples, len(grouped)), random_state=42)\n",
    "    \n",
    "    print(f\"\\nEvaluating on {len(sample_data)} images...\\n\")\n",
    "    \n",
    "    results = []\n",
    "    for idx, row in sample_data.iterrows():\n",
    "        img_name = row[\"image_name\"]\n",
    "        ground_truth_captions = row[\"comment\"]\n",
    "        \n",
    "        img_path = Path(images_dir) / img_name\n",
    "        \n",
    "        if not img_path.exists():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Generate caption\n",
    "            generated_caption = generate_caption(img_path)\n",
    "            \n",
    "            results.append({\n",
    "                'image': img_name,\n",
    "                'generated': generated_caption,\n",
    "                'ground_truth': ground_truth_captions[0]  # First ground truth\n",
    "            })\n",
    "            \n",
    "            print(f\"Image: {img_name}\")\n",
    "            print(f\"Generated:    {generated_caption}\")\n",
    "            print(f\"Ground truth: {ground_truth_captions[0]}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error on {img_name}: {e}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate on Flickr test set\n",
    "results = evaluate_on_flickr(\n",
    "    csv_path=csv_path,\n",
    "    images_dir=image_path,\n",
    "    num_samples=10\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4. SAVE RESULTS TO FILE\n",
    "# ============================================\n",
    "\n",
    "def save_results_to_csv(results, output_file=\"caption_results.csv\"):\n",
    "    \"\"\"Save caption results to a CSV file\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa52321d-8de1-4724-94e0-3b11799e4197",
   "metadata": {},
   "source": [
    "### __Model Evaluation with BLEU and ROUGE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6ade52d-175a-4c78-8037-303a1e623449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Fine-tuned Model...\n",
      "Loading model...\n",
      "Model loaded on cuda\n",
      "Loading dataset...\n",
      "Evaluating on 100 images...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:45<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Number of images evaluated: 100\n",
      "\n",
      "BLEU Scores:\n",
      "  BLEU-1: 0.6094\n",
      "  BLEU-2: 0.4473\n",
      "  BLEU-3: 0.3143\n",
      "  BLEU-4: 0.2196\n",
      "\n",
      "ROUGE Scores:\n",
      "  ROUGE-1: 0.5500\n",
      "  ROUGE-2: 0.3077\n",
      "  ROUGE-L: 0.4944\n",
      "============================================================\n",
      "\n",
      "Detailed results saved to finetuned_results.csv\n",
      "\n",
      "\n",
      "Evaluating Baseline Model...\n",
      "Loading model...\n",
      "Model loaded on cuda\n",
      "Loading dataset...\n",
      "Evaluating on 100 images...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:23<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Number of images evaluated: 100\n",
      "\n",
      "BLEU Scores:\n",
      "  BLEU-1: 0.5696\n",
      "  BLEU-2: 0.4086\n",
      "  BLEU-3: 0.2902\n",
      "  BLEU-4: 0.1982\n",
      "\n",
      "ROUGE Scores:\n",
      "  ROUGE-1: 0.4741\n",
      "  ROUGE-2: 0.2704\n",
      "  ROUGE-L: 0.4505\n",
      "============================================================\n",
      "\n",
      "Detailed results saved to baseline_results.csv\n",
      "\n",
      "============================================================\n",
      "COMPARISON: Fine-tuned vs Baseline\n",
      "============================================================\n",
      "\n",
      "BLEU-1:\n",
      "  Baseline:    0.5696\n",
      "  Fine-tuned:  0.6094\n",
      "  Improvement: +7.00%\n",
      "\n",
      "BLEU-2:\n",
      "  Baseline:    0.4086\n",
      "  Fine-tuned:  0.4473\n",
      "  Improvement: +9.47%\n",
      "\n",
      "BLEU-3:\n",
      "  Baseline:    0.2902\n",
      "  Fine-tuned:  0.3143\n",
      "  Improvement: +8.33%\n",
      "\n",
      "BLEU-4:\n",
      "  Baseline:    0.1982\n",
      "  Fine-tuned:  0.2196\n",
      "  Improvement: +10.81%\n",
      "\n",
      "ROUGE-1:\n",
      "  Baseline:    0.4741\n",
      "  Fine-tuned:  0.5500\n",
      "  Improvement: +16.01%\n",
      "\n",
      "ROUGE-2:\n",
      "  Baseline:    0.2704\n",
      "  Fine-tuned:  0.3077\n",
      "  Improvement: +13.79%\n",
      "\n",
      "ROUGE-L:\n",
      "  Baseline:    0.4505\n",
      "  Fine-tuned:  0.4944\n",
      "  Improvement: +9.73%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# ============================================\n",
    "# 1. LOAD MODEL\n",
    "# ============================================\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load the fine-tuned model\"\"\"\n",
    "    print(\"Loading model...\")\n",
    "    processor = BlipProcessor.from_pretrained(model_path)\n",
    "    model = BlipForConditionalGeneration.from_pretrained(model_path)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded on {device}\")\n",
    "    return model, processor, device\n",
    "\n",
    "\n",
    "def generate_caption(model, processor, device, image_path, max_length=50, num_beams=5):\n",
    "    \"\"\"Generate caption for an image\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length, num_beams=num_beams)\n",
    "    \n",
    "    caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2. BLEU SCORE CALCULATION\n",
    "# ============================================\n",
    "\n",
    "def calculate_bleu_scores(reference_captions, generated_caption):\n",
    "    \"\"\"\n",
    "    Calculate BLEU scores (BLEU-1, BLEU-2, BLEU-3, BLEU-4)\n",
    "    \n",
    "    Args:\n",
    "        reference_captions: List of reference captions (ground truth)\n",
    "        generated_caption: Generated caption string\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with BLEU-1 through BLEU-4 scores\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    references = [caption.lower().split() for caption in reference_captions]\n",
    "    candidate = generated_caption.lower().split()\n",
    "    \n",
    "    # Smoothing function to handle edge cases\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    # Calculate individual BLEU scores\n",
    "    bleu_scores = {\n",
    "        'BLEU-1': sentence_bleu(references, candidate, weights=(1, 0, 0, 0), smoothing_function=smoothing),\n",
    "        'BLEU-2': sentence_bleu(references, candidate, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing),\n",
    "        'BLEU-3': sentence_bleu(references, candidate, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing),\n",
    "        'BLEU-4': sentence_bleu(references, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing),\n",
    "    }\n",
    "    \n",
    "    return bleu_scores\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 3. ROUGE SCORE CALCULATION\n",
    "# ============================================\n",
    "\n",
    "def calculate_rouge_scores(reference_captions, generated_caption):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L)\n",
    "    \n",
    "    Args:\n",
    "        reference_captions: List of reference captions\n",
    "        generated_caption: Generated caption string\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with ROUGE scores\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Calculate ROUGE against each reference and take the maximum\n",
    "    all_scores = []\n",
    "    for ref in reference_captions:\n",
    "        scores = scorer.score(ref, generated_caption)\n",
    "        all_scores.append(scores)\n",
    "    \n",
    "    # Aggregate scores (take max F1 for each metric)\n",
    "    rouge_scores = {\n",
    "        'ROUGE-1': max(s['rouge1'].fmeasure for s in all_scores),\n",
    "        'ROUGE-2': max(s['rouge2'].fmeasure for s in all_scores),\n",
    "        'ROUGE-L': max(s['rougeL'].fmeasure for s in all_scores),\n",
    "    }\n",
    "    \n",
    "    return rouge_scores\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4. EVALUATE ON FLICKR DATASET\n",
    "# ============================================\n",
    "\n",
    "def evaluate_model(model_path, csv_path, images_dir, num_samples=None, output_file=\"evaluation_results.csv\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on Flickr30k dataset\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to fine-tuned model\n",
    "        csv_path: Path to Flickr30k CSV file\n",
    "        images_dir: Directory containing images\n",
    "        num_samples: Number of samples to evaluate (None = all)\n",
    "        output_file: Where to save detailed results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load model\n",
    "    model, processor, device = load_model(model_path)\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(csv_path, sep=\"|\")\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Group captions by image\n",
    "    grouped = df.groupby(\"image_name\")[\"comment\"].apply(list).reset_index()\n",
    "    \n",
    "    # Sample if needed\n",
    "    if num_samples:\n",
    "        grouped = grouped.sample(min(num_samples, len(grouped)), random_state=42)\n",
    "    \n",
    "    print(f\"Evaluating on {len(grouped)} images...\\n\")\n",
    "    \n",
    "    # Storage for results\n",
    "    all_results = []\n",
    "    all_bleu_scores = {f'BLEU-{i}': [] for i in range(1, 5)}\n",
    "    all_rouge_scores = {'ROUGE-1': [], 'ROUGE-2': [], 'ROUGE-L': []}\n",
    "    \n",
    "    # Evaluate each image\n",
    "    for idx, row in tqdm(grouped.iterrows(), total=len(grouped), desc=\"Evaluating\"):\n",
    "        img_name = row[\"image_name\"]\n",
    "        reference_captions = row[\"comment\"]\n",
    "        \n",
    "        img_path = Path(images_dir) / img_name\n",
    "        \n",
    "        if not img_path.exists():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Generate caption\n",
    "            generated_caption = generate_caption(model, processor, device, img_path)\n",
    "            \n",
    "            # Calculate BLEU scores\n",
    "            bleu_scores = calculate_bleu_scores(reference_captions, generated_caption)\n",
    "            \n",
    "            # Calculate ROUGE scores\n",
    "            rouge_scores = calculate_rouge_scores(reference_captions, generated_caption)\n",
    "            \n",
    "            # Store scores\n",
    "            for key, value in bleu_scores.items():\n",
    "                all_bleu_scores[key].append(value)\n",
    "            \n",
    "            for key, value in rouge_scores.items():\n",
    "                all_rouge_scores[key].append(value)\n",
    "            \n",
    "            # Store detailed results\n",
    "            result = {\n",
    "                'image': img_name,\n",
    "                'generated_caption': generated_caption,\n",
    "                'reference_caption_1': reference_captions[0],\n",
    "                **bleu_scores,\n",
    "                **rouge_scores\n",
    "            }\n",
    "            all_results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate average scores\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nNumber of images evaluated: {len(all_results)}\\n\")\n",
    "    \n",
    "    print(\"BLEU Scores:\")\n",
    "    for key in all_bleu_scores:\n",
    "        avg_score = np.mean(all_bleu_scores[key])\n",
    "        print(f\"  {key}: {avg_score:.4f}\")\n",
    "    \n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    for key in all_rouge_scores:\n",
    "        avg_score = np.mean(all_rouge_scores[key])\n",
    "        print(f\"  {key}: {avg_score:.4f}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nDetailed results saved to {output_file}\")\n",
    "    \n",
    "    # Return summary statistics\n",
    "    summary = {\n",
    "        'num_samples': len(all_results),\n",
    "        **{f'avg_{key}': np.mean(all_bleu_scores[key]) for key in all_bleu_scores},\n",
    "        **{f'avg_{key}': np.mean(all_rouge_scores[key]) for key in all_rouge_scores}\n",
    "    }\n",
    "    \n",
    "    return summary, results_df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 5. COMPARE WITH BASELINE\n",
    "# ============================================\n",
    "\n",
    "def compare_with_baseline(finetuned_path, csv_path, images_dir, num_samples=100):\n",
    "    \"\"\"Compare fine-tuned model with original BLIP baseline\"\"\"\n",
    "    \n",
    "    print(\"Evaluating Fine-tuned Model...\")\n",
    "    ft_summary, _ = evaluate_model(finetuned_path, csv_path, images_dir, num_samples, \"finetuned_results.csv\")\n",
    "    \n",
    "    print(\"\\n\\nEvaluating Baseline Model...\")\n",
    "    baseline_summary, _ = evaluate_model(\"Salesforce/blip-image-captioning-base\", csv_path, images_dir, num_samples, \"baseline_results.csv\")\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARISON: Fine-tuned vs Baseline\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    metrics = ['avg_BLEU-1', 'avg_BLEU-2', 'avg_BLEU-3', 'avg_BLEU-4', \n",
    "               'avg_ROUGE-1', 'avg_ROUGE-2', 'avg_ROUGE-L']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        ft_score = ft_summary[metric]\n",
    "        baseline_score = baseline_summary[metric]\n",
    "        improvement = ((ft_score - baseline_score) / baseline_score) * 100\n",
    "        \n",
    "        print(f\"\\n{metric.replace('avg_', '')}:\")\n",
    "        print(f\"  Baseline:    {baseline_score:.4f}\")\n",
    "        print(f\"  Fine-tuned:  {ft_score:.4f}\")\n",
    "        print(f\"  Improvement: {improvement:+.2f}%\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "model_path = \"./blip-finetuned-flickr\"\n",
    "\n",
    "compare_with_baseline(\n",
    "    finetuned_path=model_path,\n",
    "    csv_path=csv_path,\n",
    "    images_dir=image_path,\n",
    "    num_samples=100\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m136",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m136"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
