{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9354c5c1",
   "metadata": {},
   "source": [
    "# Problem 3. Steps to Design a Feature Selection Algorithm (Module 3)\n",
    "40 Points Total<br><br>\n",
    "\n",
    "In the lecture notes an example of ranking and selecting the top attributes (features) from data is provided. This example will be the starting point for ranking your features of interest. <br>\n",
    "\n",
    "In this problem you will be designing an intelligent algorithm for feature selection and ranking by identifying the most relevant features from a dataset that contribute most significantly to the outcome of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb89ff5",
   "metadata": {},
   "source": [
    "# Fill before submitting\n",
    "- **Student:** `Zach Hatzenbeller`\n",
    "- **Course:** `Data Science Modeling & Analytics`\n",
    "- **Homework:** `HW #1`\n",
    "- **Date:** `2025-09-14`\n",
    "- **Instructor:** `Ben Rodriguez, PhD`\n",
    "\n",
    "---\n",
    "\n",
    "## What to Submit in your `.ipynb`\n",
    "Your notebook must include, in this order:\n",
    "\n",
    "1. **Cover Block** — Name, course, HW #, date.  \n",
    "2. **README (Execution & Setup)** — Python version; required packages + install steps; dataset source + download/use instructions; end‑to‑end run steps; any hardware notes (GPU/CPU).  \n",
    "3. **Adjustable Inputs** — A single, clearly marked code cell where we can change paths, seeds, and key hyperparameters.  \n",
    "4. **Problem Sections** — Each problem and sub‑part clearly labeled (e.g., “Problem 1 (a)”).  \n",
    "5. **Results, Summary & Conclusions** — Your takeaways, trade‑offs, limitations.  \n",
    "6. **References & Attributions** — Cite datasets, code you reused, articles, and **any AI tools** used (and how).\n",
    "\n",
    "> **One file only**. The notebook must run **top‑to‑bottom** with no errors.\n",
    "\n",
    "---\n",
    "\n",
    "## How You’re Graded (what “full credit” looks like)\n",
    "\n",
    "**1) Completeness & Problem Coverage (20%)**  \n",
    "<div style=\"margin-left: 40px\"> To earn full points, students must ensure that all parts of the assignment, including sub-questions, are fully answered. Both qualitative and quantitative components should be addressed where required, and any coding tasks must be implemented completely without omissions. </div>\n",
    "\n",
    "**2) Writing Quality, Technical Accuracy & Justification (20%)**  \n",
    "<div style=\"margin-left: 40px\"> Writing should be clear, concise, and demonstrate graduate-level quality. All technical content must be correct, and reasoning should be sound and well-supported. Students are expected to justify their design choices and conclusions with logical arguments that reflect a strong understanding of the material. </div>   \n",
    "\n",
    "**3) Quantitative Work (0% on this HW)**  \n",
    "<div style=\"margin-left: 40px\"> Assignments should clearly state all assumptions before attempting solutions. Derivations and calculations must be shown step by step, either in Markdown cells or through annotated code. Final results should be presented with appropriate units and precision, ensuring they are easy to interpret and technically correct. </div>\n",
    " \n",
    "**4) Code Quality, Documentation & Execution (30%)**  \n",
    "<div style=\"margin-left: 40px\"> Code must run from top to bottom without errors, avoiding “Traceback” or other runtime issues. Programs should follow best practices for naming, formatting, and organization, with descriptive variables and functions. Meaningful comments should be included to explain key logic, making the code both efficient and easy to follow. </div>\n",
    "\n",
    "**5) Examples, Test Cases & Visuals (20%)**  \n",
    "<div style=\"margin-left: 40px\"> Students should include realistic examples and test cases that demonstrate program functionality, with outputs clearly labeled. Figures and tables must be properly titled, captioned, and have labeled axes. For machine learning tasks, particularly those with imbalanced datasets such as Credit Card Fraud or NSL-KDD, evaluation metrics must go beyond simple accuracy and include measures like precision, recall, F1-score, and ROC or PR curves. </div>\n",
    "\n",
    "**6) Notebook README & Reproducibility (10%)**  \n",
    "<div style=\"margin-left: 40px\"> Each notebook must include a README section containing the Python version, a list of required packages with installation instructions, dataset details with download information, and complete steps to run the notebook. The work should be fully reproducible on another system, with seeds set for consistency and relative paths used instead of system-dependent absolute paths. </div>\n",
    "\n",
    "---\n",
    "\n",
    "## README (Execution & Setup)\n",
    "\n",
    "**Use this section to make your notebook reproducible.**\n",
    "\n",
    "- **Python version:** `3.11.1`\n",
    "- **Required packages:** `numpy`, `pandas`, `scikit-learn`, `matplotlib`\n",
    "- **Install instructions (if non-standard):**\n",
    "  ```bash\n",
    "  pip install numpy pandas scikit-learn matplotlib\n",
    "  ```\n",
    "- **Datasets used:**\n",
    "  - `mosapabdelghany/medical-insurance-cost-dataset` dataset was downloaded directly from kaggle with kagglehub\n",
    "  - All steps are in order and will clean/transform the dataset if necessary\n",
    "- **How to run this notebook:**\n",
    "  1. Run all cells in order (Kernel → Restart & Run All).\n",
    "  2. Verify that all outputs match those in the **Sample Tests** section.\n",
    "  3. Ensure figures and tables render correctly.\n",
    "\n",
    "**README hint**: Place `creditcard.csv` in `./data/` (or set `DATA_PATH` below). The dataset can be downloaded from Kaggle. Due to size limits, keep only relative paths in this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## Adjustable Inputs Example\n",
    "```python\n",
    "# === Adjustable Inputs (edit here) ===\n",
    "DATA_PATH = \"path/to/data.csv\"     # use relative paths\n",
    "RANDOM_SEED = 42\n",
    "# Isolation Forest\n",
    "IF_N_ESTIMATORS = 200\n",
    "IF_MAX_SAMPLES = \"auto\"\n",
    "IF_CONTAMINATION = 0.01            # will be swept below\n",
    "# LOF\n",
    "LOF_N_NEIGHBORS = 20\n",
    "LOF_CONTAMINATION = 0.01           # will be swept below\n",
    "# Contamination sweep values (example)\n",
    "CONTAMINATION_GRID = [0.001, 0.005, 0.01, 0.02, 0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f63e2",
   "metadata": {},
   "source": [
    "## Problem Statement for Problem 3: Steps to Design a Feature Selection Algorithm\n",
    "\n",
    "Use the following steps for your algorithm:\n",
    "* Analyze the Data\n",
    "* Define Selection Criteria\n",
    "* Choose a Feature Selection Method\n",
    "* Implement the Selection Algorithm\n",
    "* Evaluate Feature Importance\n",
    "* Iterate and Optimize\n",
    "\n",
    "You will need to implement 2 of the following algorithms:\n",
    "1) Recursive Feature Elimination (RFE)\n",
    "   - Uses an external estimator to weigh the importance of features and recursively remove the least important ones.\n",
    "2) Forward/Backward Selection\n",
    "   - Iteratively adds/removes features based on model performance.\n",
    "3) Decision Trees (e.g., Random Forest, Gradient Boosting Trees)\n",
    "   - These models provide feature importance scores as a by-product of model training.\n",
    "4) Principal Component Analysis (PCA)\n",
    "   - For continuous variables, reduces dimensionality while preserving variance.\n",
    "5) Linear Discriminant Analysis (LDA)\n",
    "   - Useful for classification problems to find the feature combination that best separates classes.\n",
    "6) Feature Importance from Neural Networks\n",
    "   - Using techniques like permutation importance to assess the impact of each feature on neural network predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7df1532",
   "metadata": {},
   "source": [
    "# (a) [2.5 points] Analyze the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1646250",
   "metadata": {},
   "source": [
    "## Type your analysis of the data here ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e87cc1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\zhatz\\.cache\\kagglehub\\datasets\\mosapabdelghany\\medical-insurance-cost-dataset\\versions\\1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In your analysis of the data consider the following: ##\n",
    "# 1) Analyze the nature of the data (numerical, categorical, time-series, etc.).\n",
    "# 2) Determine the type of problem (classification, regression).\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mosapabdelghany/medical-insurance-cost-dataset\")\n",
    "df = pd.read_csv(Path(path, \"insurance.csv\"))\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a73560",
   "metadata": {},
   "source": [
    "### One hot encode columns that are categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cf75ed5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>charges</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>smoker_no</th>\n",
       "      <th>smoker_yes</th>\n",
       "      <th>region_northeast</th>\n",
       "      <th>region_northwest</th>\n",
       "      <th>region_southeast</th>\n",
       "      <th>region_southwest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>16884.92400</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>1725.55230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>4449.46200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>21984.47061</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>3866.85520</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     bmi  children      charges  sex_female  sex_male  smoker_no  \\\n",
       "0   19  27.900         0  16884.92400           1         0          0   \n",
       "1   18  33.770         1   1725.55230           0         1          1   \n",
       "2   28  33.000         3   4449.46200           0         1          1   \n",
       "3   33  22.705         0  21984.47061           0         1          1   \n",
       "4   32  28.880         0   3866.85520           0         1          1   \n",
       "\n",
       "   smoker_yes  region_northeast  region_northwest  region_southeast  \\\n",
       "0           1                 0                 0                 0   \n",
       "1           0                 0                 0                 1   \n",
       "2           0                 0                 0                 1   \n",
       "3           0                 0                 1                 0   \n",
       "4           0                 0                 1                 0   \n",
       "\n",
       "   region_southwest  \n",
       "0                 1  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = pd.get_dummies(df)*1 # multiply by 1 to get numeric binary for each true false column\n",
    "encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1869bf2b",
   "metadata": {},
   "source": [
    "# (b) [2.5 points] Define Selection Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a38a2d",
   "metadata": {},
   "source": [
    "## Type your selection criteria here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683e2efa",
   "metadata": {},
   "source": [
    "The criteria to select a feature will be based on statistical significance. I will implement two feature selection methods. The first being recursive feature selection which will recursively eliminate features from the dataset until a certain number of features are reached. I plan to use linear regression to rank the importance of these features based on the regression coefficients. \n",
    "\n",
    "The second method for feature selection will be principal component analysis to reduce the number of features in the space. The criteria for selecting how many components to use will depend on the variance explained from those features. Ideally the variance explained will be greater than 90% of the original data. This would provide high enough variance to effectively build a model while reducing the features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee707520",
   "metadata": {},
   "source": [
    "# (c) [2.5 points] Choose a Feature Selection Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f9f43",
   "metadata": {},
   "source": [
    "## Type the chosen feature selection method here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f878ba",
   "metadata": {},
   "source": [
    "- Recursive Feature Selection: will recusively select features using linear regression to determine the importance of each feature at each iteration\n",
    "- Principal Component Analysis: will reduce the number of features in the dataset to a number of components either specified by the user or based on the variance explained by the reduction. Reducing the dimensionality of the features will allow for only the most important variance to be captured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96909d7d",
   "metadata": {},
   "source": [
    "# (d) [20 points] Implement the Selection Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f63ee9",
   "metadata": {},
   "source": [
    "## Type your implementation of the selection algorithm here ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "576bd356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     bmi  children  smoker_yes\n",
       "0   19  27.900         0           1\n",
       "1   18  33.770         1           0\n",
       "2   28  33.000         3           0\n",
       "3   33  22.705         0           0\n",
       "4   32  28.880         0           0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Number of Components: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.88105705,  2.8175999 , -0.75317391,  1.75760484],\n",
       "       [ 0.82113249, -1.30845864,  1.63114951, -0.58722786],\n",
       "       [ 0.81408993, -1.34196252,  1.61331106, -0.54154408],\n",
       "       ...,\n",
       "       [-1.07826824,  0.75390532,  2.09189278, -0.56093002],\n",
       "       [-1.63890056,  0.45477051, -0.3789003 ,  1.60874176],\n",
       "       [ 0.75432539,  2.83206643, -1.06794547, -1.29748481]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## In implementing the selection algorithm consider the following: ##\n",
    "# 1) Develop logic to evaluate features based on the chosen method.\n",
    "# 2) Ensure scalability and efficiency, especially for large datasets.\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class RecursiveFeatureSelection:\n",
    "\n",
    "    def __init__(self, X: pd.DataFrame, y: pd.Series, num_features: int, scaled: bool = False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.columns = list(self.X.columns)\n",
    "        self.num_features = num_features\n",
    "        self.scaled = scaled\n",
    "        \n",
    "    def evaluate(self):\n",
    "        # Create the pipeline\n",
    "        len_col = len(self.X.columns)\n",
    "        iterations = (len_col - self.num_features) if self.num_features < len_col else 1\n",
    "        \n",
    "        if not self.scaled:\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),  # Step 1: Standardize the features\n",
    "                ('regressor', LinearRegression()) # Step 2: Apply Linear Regression\n",
    "            ])\n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "                ('regressor', LinearRegression()) # Step 2: Apply Linear Regression\n",
    "            ])\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            X = self.X.values\n",
    "            y = self.y.values\n",
    "            reg = pipeline.fit(X, y)\n",
    "            coef = reg[\"regressor\"].coef_\n",
    "            min_val = np.argmin(coef)\n",
    "            remove_col = self.columns[min_val]\n",
    "            self.X = self.X.loc[:, self.X.columns != remove_col]\n",
    "            self.columns.remove(remove_col)\n",
    "        \n",
    "        X = self.X.values\n",
    "        y = self.y.values\n",
    "        reg = pipeline.fit(X, y)\n",
    "        coef = reg[\"regressor\"].coef_\n",
    "        \n",
    "        return self.X, coef\n",
    "\n",
    "class PCAFeatureSelection:\n",
    "\n",
    "    def __init__(self, X: pd.DataFrame, features: int = -1, var_explained: float = 0.9, scaled: bool = False):\n",
    "        self.X = X\n",
    "        self.features = features\n",
    "        self.var_explained = var_explained\n",
    "        self.scaled = scaled\n",
    "\n",
    "    def evaluate(self):\n",
    "        num_features = len(self.X.columns)\n",
    "\n",
    "        if not self.scaled:\n",
    "            self.X = StandardScaler().fit_transform(self.X)\n",
    "        else:\n",
    "            self.X = self.X.values\n",
    "        \n",
    "        if self.features > 0:\n",
    "            self.X = PCA(n_components=self.features).fit_transform(self.X)\n",
    "        \n",
    "        if self.features < 0:\n",
    "            for comp in range(1, num_features-1):\n",
    "                pca_var = PCA(n_components=comp).fit(self.X)\n",
    "                var_ratio = sum(pca_var.explained_variance_ratio_)\n",
    "                if var_ratio >= self.var_explained:\n",
    "                    self.features = comp\n",
    "                    break\n",
    "\n",
    "            self.X = PCA(n_components=self.features).fit_transform(self.X)\n",
    "\n",
    "        return self.X, self.features\n",
    "        \n",
    "# Recursive feature selection\n",
    "X = encoded.loc[:, encoded.columns != \"charges\"]\n",
    "y = encoded.loc[:, encoded.columns == \"charges\"]\n",
    "rfs = RecursiveFeatureSelection(X, y, num_features=4, scaled=False)\n",
    "df_features, coef = rfs.evaluate()\n",
    "display(df_features.head())\n",
    "\n",
    "# PCA feature selection\n",
    "X = encoded.loc[:, encoded.columns != \"charges\"]\n",
    "y = encoded.loc[:, encoded.columns == \"charges\"]\n",
    "pca = PCAFeatureSelection(X, features=4, var_explained=0.9, scaled=False)\n",
    "df_decomp, num_comps = pca.evaluate()\n",
    "print(f\"PCA Number of Components: {num_comps}\")\n",
    "display(df_decomp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b1a6b",
   "metadata": {},
   "source": [
    "# (e) [10 points] Evaluate Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab7744",
   "metadata": {},
   "source": [
    "## Type your evaluation of feature importance here ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c7d919bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features before features selection: 11\n",
      "Pre-feature selection Cross Validation R2 Value:\n",
      "Fold 0: 0.760\n",
      "Fold 1: 0.701\n",
      "Fold 2: 0.778\n",
      "Fold 3: 0.735\n",
      "Fold 4: 0.756\n",
      "\n",
      "Number of Features post-Recursion: 6\n",
      "Recursive feature selection Cross Validation R2 Value:\n",
      "Fold 0: 0.763\n",
      "Fold 1: 0.707\n",
      "Fold 2: 0.779\n",
      "Fold 3: 0.733\n",
      "Fold 4: 0.756\n",
      "Important Features: ['age', 'bmi', 'children', 'smoker_yes', 'region_northeast', 'region_northwest']\n",
      "\n",
      "Number of Features post-PCA: 8\n",
      "PCA feature selection Cross Validation R2 Value:\n",
      "Fold 0: 0.761\n",
      "Fold 1: 0.706\n",
      "Fold 2: 0.778\n",
      "Fold 3: 0.733\n",
      "Fold 4: 0.756\n"
     ]
    }
   ],
   "source": [
    "## In evaluating feature importance consider the following: ##\n",
    "# 1) Assess the impact of selected features on model performance.\n",
    "# 2) Use techniques like cross-validation to avoid overfitting.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Split dataset into train and test\n",
    "X = encoded.loc[:, encoded.columns != \"charges\"]\n",
    "y = encoded.loc[:, encoded.columns == \"charges\"]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Before feature selection\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Step 1: Standardize the features\n",
    "    ('regressor', LinearRegression()) # Step 2: Apply Linear Regression\n",
    "])\n",
    "scores = cross_val_score(estimator=pipeline, X=X, y=y, cv=5, scoring='r2')\n",
    "print(f\"Number of features before features selection: {X.shape[1]}\")\n",
    "print(\"Pre-feature selection Cross Validation R2 Value:\")\n",
    "for idx, r2 in enumerate(list(scores)):\n",
    "    print(f\"Fold {idx}: {r2:0.03f}\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "rfs = RecursiveFeatureSelection(X, y, num_features=6, scaled=False)\n",
    "df_features, coef = rfs.evaluate()\n",
    "pipeline = Pipeline([\n",
    "    ('regressor', LinearRegression()) # Step 2: Apply Linear Regression\n",
    "])\n",
    "scores = cross_val_score(estimator=pipeline, X=df_features, y=y, cv=5, scoring='r2')\n",
    "print(f\"Number of Features post-Recursion: {df_features.shape[1]}\")\n",
    "print(\"Recursive feature selection Cross Validation R2 Value:\")\n",
    "for idx, r2 in enumerate(list(scores)):\n",
    "    print(f\"Fold {idx}: {r2:0.03f}\")\n",
    "print(f\"Important Features: {list(df_features.columns)}\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "pca = PCAFeatureSelection(X, features=-1, var_explained=0.95, scaled=False)\n",
    "df_decomp, num_comps = pca.evaluate()\n",
    "pipeline = Pipeline([\n",
    "    ('regressor', LinearRegression()) # Step 2: Apply Linear Regression\n",
    "])\n",
    "scores = cross_val_score(estimator=pipeline, X=df_decomp, y=y, cv=5, scoring='r2')\n",
    "print(f\"Number of Features post-PCA: {num_comps}\")\n",
    "print(\"PCA feature selection Cross Validation R2 Value:\")\n",
    "for idx, r2 in enumerate(list(scores)):\n",
    "    print(f\"Fold {idx}: {r2:0.03f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6cef0",
   "metadata": {},
   "source": [
    "# (f) [2.5 points] Iterate and Optimize the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6e8a60",
   "metadata": {},
   "source": [
    "## Type your description of iterating and optimizing the algorithm here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a260e1d0",
   "metadata": {},
   "source": [
    "To further optimize each algorithm I would do the following:\n",
    "- Recursive feature selection: I would iterate through range of features that I would want to reduce the model to and then select the number of features that performs best in cross validation.\n",
    "- PCA feature selection: I would iterate through a percent of variance explained by the data to select a set number of components that achieves that variance explained and will lead to the highest cross validation scores among the folds.\n",
    "\n",
    "In this feature selection we ideally want to go for a simple model that retains or improves accuracy over a model using all features. Generally feature selection shows vast improvements when the data has a high number of dimensions. The dataset we work with in this notebook has 11 features which generally is not considered high. Using our feature selection methods though, we do see an improvement in most of the cross validation folds. This showcases that we can still improve a model even if the number of dimensions is not severly high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d411f7c",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Christopher M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995.<br><br>\n",
    "[2] Christopher Bishop. Pattern Recognition and Machine Learning. Springer, 2006. isbn: 0387310738.<br><br>\n",
    "[3] Barry J. Shepherd C. Wayne Brown. Graphics File Formats: Reference and Guide. Manning\n",
    "Publications, 1995. isbn: 1884777007.<br><br>\n",
    "[4] Thomas H. Cormen et al. Introduction to Algorithms. 3rd. MIT Press, 2009. isbn: 780262033848.<br><br>\n",
    "[5] W. R. Dillon and M. Goldstein. Multivariate Analysis Method and Applications. New York, NY:\n",
    "John Wiley Sons, Inc, 1984.<br><br>\n",
    "[6] Richard O. Duda, Peter E. Hart, and David G. Stork. Pattern Classification. 2nd. Wiley-\n",
    "Interscience, 2000.<br><br>\n",
    "[7] Duin et al. PRTools. https://cmp.felk.cvut.cz/cmp/software/stprtool/index.html.<br><br>\n",
    "[8] L. Euler. “Nova Acta Acad. Sci. Petrop”. In: (1960).<br><br>\n",
    "[9] R.A. Fisher. “The use of Multiple Measurements in Taxonomic Problems”. In: Proceedings of\n",
    "Annals of Eugenics 7 (1936), pp. 179–188.<br><br>\n",
    "[10] Vojtech Franc and Vaclav Hlavac. Statistical Pattern Recognition Toolbox. https://cmp.felk.\n",
    "cvut.cz/cmp/software/stprtool/index.html.<br><br>\n",
    "[11] Keinosuke Fukunaga. Introduction to Statistical Pattern Recognition. 1st. Academic Press, 1972.\n",
    "isbn: 0122698509.<br><br>\n",
    "[12] Keinosuke Fukunaga. Introduction to Statistical Pattern Recognition. 2nd. Academic Press, 1990.\n",
    "isbn: 0122698517.<br><br>\n",
    "[13] Herman H. Goldstine. A History of Numerical Analysis from the 16th through the 19th Century.\n",
    "Springer New York, 1977. isbn: 978-0-387-90277-7.<br><br>\n",
    "[14] H. Hotelling. “Analysis of a complex of statistical variables into principal components”. In: Jour-\n",
    "nal of Educational Psychology 24 (1933), pp. 417–441.<br><br>\n",
    "[15] Averill Law. Simulation Modeling and Analysis. 5th. Mcgraw-hill Series in Industrial Engineering\n",
    "and Management, 2014.<br><br>\n",
    "[16] Machine Learning at Waikato University. https://www.cs.waikato.ac.nz/~ml/index.html.<br><br>\n",
    "[17] James D. Murry and William vanRyper. Encyclopedia of Graphics File Formats: The Com-\n",
    "plete Reference on CD-ROM with Links to Internet Resources. 2nd. O’Reilly Media, 1996. isbn:\n",
    "1565921615.<br><br>\n",
    "[18] F. Pedregosa et al. “Scikit-learn: Machine Learning in Python”. In: Journal of Machine Learning\n",
    "Research 12 (2011), pp. 2825–2830.<br><br>\n",
    "[19] Casey J. Richards et al. “Multimodal data fusion using signal/image processing methods for\n",
    "multi-class machine learning”. In: Signal Processing, Sensor/Information Fusion, and Target\n",
    "Recognition XXXII. Ed. by Ivan Kadar, Erik P. Blasch, and Lynne L. Grewe. Vol. 12547. Inter-\n",
    "national Society for Optics and Photonics. SPIE, 2023, 125470N. doi: 10.1117/12.2664987.\n",
    "url: https://doi.org/10.1117/12.2664987.<br><br>\n",
    "[20] Benjamin M. Rodriguez. “Multi-Class Classification for Identifying JPEG Steganography Em-\n",
    "bedding Methods”. PhD thesis. Air Force Institute of Technology, 2008. url: https://scholar.\n",
    "afit.edu/cgi/viewcontent.cgi?article=3642&context=etd.<br><br>\n",
    "[21] Stuart Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th. Prentice Hall,\n",
    "2020.<br><br>\n",
    "[22] Amir Saeed et al. “Reinforcement learning application to satellite constellation sensor tasking”.\n",
    "In: Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications V.\n",
    "Ed. by Latasha Solomon and Peter J. Schwartz. Vol. 12538. International Society for Optics and\n",
    "Photonics. SPIE, 2023, 125381B. doi: 10.1117/12.2664346. url: https://doi.org/10.1117/\n",
    "12.2664346.<br><br>\n",
    "[23] C. E. Shannon. “Programming a Computer for Playing Chess”. In: Philosophical Magazine.\n",
    "7th ser. 41.314 (1950).<br><br>\n",
    "[24] Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. MIT Press,\n",
    "2018.<br><br>\n",
    "[25] Sergios Theodoridis and Konstantinos Koutroumbas. Pattern Recognition. 3rd. Academic Press,\n",
    "2006. isbn: 0123695317.<br><br>\n",
    "[26] Alan M. Turing. “Computing Machinery and Intelligence”. In: Mind 59.236 (1950), pp. 433 –460.<br><br>\n",
    "[27] P. Winston. Artificial Intelligence. 3rd. Pearson, 1992.<br><br>"
   ]
  }
 ],
 "metadata": {
  "autoupdated": {
   "by": "ChatGPT – HW1 templating utility",
   "updated_at": "2025-08-16T01:47:56.304321"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "title": "DSMA Homework Template"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
